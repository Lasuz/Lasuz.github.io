{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/1969/12/31/hello-world/"},{"title":"My First Test","text":"Trial1I always wanted to make a own website not using the commercially available software tools. here is it.","link":"/2023/09/08/my-first-test/"},{"title":"Intensity Normalization in Medical Imaging","text":"Image Intensity Normalization in Medical ImagingDo you use Min-Max normalization in your deep learning pipeline and have you thought about the why? Many preprocessing steps become second nature and their use does not make us think twice, one example is image intensity normalization. However, sometimes it is good to take a step back and think through it again. In this blog post, I want to show you why intensity normalization in medical imaging in particular Magnetic Resonance Imaging (MRI) differs from natural images and demonstrate established alternatives. In the first section I am focusing on MRI-specific properties and in the later, other normalization techniques are highlighted. At the end of the post, you can find a tool, where you can explore the common intensity normalization with your data. Intensity Normalization is essential. It helps our network to converge more efficiently. Fluctuations in voxel values are reduced which leads to a smoother manifold of the loss function, and hence training benefits. Also having the input in a range around 0, allow the model to update the weights more consistently because the step size is fluctuating less. It is reducing the problem of exploding gradients. Therefore, intensity normalization became a standard preprocessing step. Min-Max normalization, also known as re-scaling is the commonly used approach where the image intensity range gets transformed into the range from 0–1, which does not affect the image itself. Min-Max Normalization:normalized_value = (original_value - min_value) / (max_value - min_value) Dealing with natural images, our image intensities live between 0 and 255, and normalizing between the upper and lower bound is intuitive and well-motivated. However, intuitive motivation breaks apart when dealing with medical images in particular MRI because 1) image intensity reflects tissue type, 2) image intensity is relative and 3) intensity range is not bounded.Let’s address these three points separately.Image Intensity Reflects Tissue Type: Conventional MRI has exceptional soft tissue contrast and it allows the radiologist to differentiate between different tissue types. The voxel intensity refers to specific tissue composition. Looking at the histograms of the entire volume, allows us to see that certain tissues are more apparent than others. This is different compared to natural images where the intensity value reflects the brightness of the object.Histogram for a skull stripped 3D T1 out of the ADNI dataset. The horizontal lines added to the histogram reflect the most common tissue at a specific location. A tissue category such as white and gray matter is not reflected by a single number rather it is a range, leading to the visualized histogram. Note the maximum intensity value in the image is 8974, it is much higher than common tissue represented in the brain.MR Image Intensity is Relative: Conventional MRI is qualitative which means, that even though the intensity value refers to a specific tissue, a repeated MRI scan will obtain different voxel intensities. However, the absolute intensity does not matter in MRI because MR images benefit from the contrast between the different tissues and not the absolute value.The intensity value itself has no physical meaning rather is the measured current in the receiver coil transformed using the Fourier transformation. The current in the receiver coil depends on many factors such as the pulse sequence, the hardware (e.g. the magnetic field strength, the coil design), the software used to do the image reconstruction and even temperature changes, as well as the body position inside the coil has an impact on the intensity among many other factors. The current at the receiver coil gets calibrated and hence led to session-specific variations. Two major contributors to absolute intensity value are the pulse sequence design and the MRI manufacturer/vendor. Below you can find a figure of three scans and their histograms acquired with the same T1 pulse sequence at a GE scanner.Histogram of 3 different subjects acquired at a GE scanner (from the ADNI dataset). Note maximum intensity value varies greatly: 8974, 5703, and 10372 for Subject A, B, and C retrospectively. Variations in the histogram shape is due to several factors, among patient-specific anatomy but also differences in the skull stripping performance. In Subject C (green), you can see a tiny bright border around the brain, which is fatty tissue and lead to a bright signal that is an artifact from not-perfect skull stripping. As you might have noticed the green image (Subject C) appears darker, which is due to the extra portion of fat which increases the range of intensity value visualized; high-intensity areas are now representing fat where previously it was white matter (the Window &amp; Level differ). The difference in histogram height arises from the varying number of voxels within the brain. The absolute intensity value varies in MRI and because neither our eyes nor the computer visualization can differentiate between thousands of grey values, adjusting the Window &amp; Level to analyze the area of interest intensity is common practice. Therefore when opening an image in a medical viewer such as fsl-lense or itk-snap, the window and level are adjusted automatically to achieve the best contrast. Whereas when plotting images using libraries such as matplotlib, the full intensity range is evenly split into bins.Image Intensity Range is not Bounded: Image intensity in MRI varies as described above. There is no real upper limit. As we saw previously some scanners have images with an absolute intensity value of 10,000 others only have intensities of 6,000. In the figure below, histograms of T1 images for different vendors are visualized. It is quite clear that those must come from a different dataset, as their shape and absolute intensity value are very different. In other words, images from different vendors have a different intensity range.Histogram from sagittal T1 brain scans of a different dataset (from a Brainhack Challenge in Campinas 2019). Not only where the MR scans acquired at different scanners but also they are orientated sagittally. Voxel intensity distribution varies greatly.Normalization Techniques When preparing MRI images for a deep learning pipeline, normalization is common practice. The goal is to move the image intensity close to 0 by maintaining the relationship between the voxels. Let’s see what Min-Max Image Normalization does to the previously shown histograms acquired at the same scanner.Min-Max Normalization for the above-shown subject. The intensity values are in the range from 0 to 1, but the white matter, gray matter, and CSF (different peaks of the histograms) have very different intensity values when placed into the network for training. The intensity values are within [0,1] but as you can see the peaks of the histogram representing the different tissues do not align any longer. The peak of white matter signal in Subject A has a value of roughly 0.5 which corresponds to gray matter in Subject B. The misalignment of the histograms after the Min-Max normalization is due to outliers such as the existence/lack of certain tissue e.g. faulty skull stripping which kept fatty tissue or scanner-dependent variables e.g. noise, pulse sequences design. Standardization of the Batch: Normalizing the whole batch by the mean and standard deviationBetter than the Min-Max Normalization performs the normalization of the whole batch at once. Statistical parameters such as the mean and standard deviation of the whole batch are used to normalize the images by subtracting the mean and dividing the image by the standard deviation. As the parameters are constant within the batch, the operation is linear and leads to the same histogram alignment as observed in the non-normalized state. Z-score:$$normalized_value = (original_value - mean_value) / (std_value)$$ Assuming a batch size of 3 containing the three above subjects. A normalization using mean and standard deviation preserves the original alignment of tissues. This type of normalization might be sufficient for many situations, especially when the dataset comes from the same side. However, variations inside the dataset (e.g. vendors, field strength), lead also to variations in the voxel intensity for specific tissues. Also, it has been widely observed that models generalize poorly on datasets acquired at different centers.Normalization versus Aligned Histograms The purpose of normalization is not to align histograms. Rather, it places the histograms into a similar intensity range. Nevertheless, we can increase the information we pass to our network by choosing the optimal normalization technique. In medical imaging, our data often comes from a relatively homogeneous distribution, for example, the dataset exists of e.g., only T1 brain MR. Most often that is the case. It is expected that similar tissues are observed within all brains, but quantity and locations differ. The alignment of histograms allows us to determine tissue-specific intensity ranges, which are comparable over all subjects. When the dataset, however, is a mixture of different body parts or different imaging techniques (CT, MRI, X-Ray). The meaning of aligned histograms might become obsolete.Importance of Aligned Histograms You may wonder why it matters that histograms are aligned; within the image, contrast, and shape are preserved in any way. However, we do not feed in one image at a time and update our weights after each iteration rather we have multiple images within a batch that is used for training. For the model to learn those variations in intensity required more data and iterations. In medical imaging, data is not bounded and often studies would benefit from additional data. Therefore, it is more important to have the available data prepared well such that distractions deviating from the actual task are minimized. Those preprocessing steps, not only include N4 bias field correction and skull striping, but also intensity normalization. A separate field in MR deep learning focuses on MR harmonization techniques, those techniques try to remove site-specific effects on the MR image with the goal to generalize the model better to different datasets. MR Harmonization incorporates normalization techniques but exceeds basic intensity normalization. How important the normalization step is, depends on the specific task and data at hand. Overall, normalization can be seen as one of the many hyper-parameters which need to be chosen to make your model succeed.Established Normalization Techniques In the field of radiomics, many studies analyzed different normalization techniques. Common alternatives which in most cases outperform Min-Max Normalization are: Z-score standardization — Standardizing the Volume by the mean and the standard deviation, does not put the values in a range between [0,1] but the data is centered around 0 with [-std, std] at [-1,1]. That “normalization” technique is of particular interest when data follows similar distribution e.g. just one body part of the same pulse sequence. Percentile Normalization normalizes to a specific percentile e.g. 2% as lower bound and 98% as upper bound. By taking the percentile, outliers are removed from the normalization. This technique is interesting if the data has outliers and the distribution within the data varies. Histogram Matching is a technique that modifies the intensity distribution of an image to match a specific target histogram. This technique was proposed by Nyul et al in 1999. Python libraries such as itk and scikit-learn have algorithms included. However, it can affect the texture of the image. White Stripe is a tissue-specific normalization technique that normalizes the image volume to a particular tissue class, in the brain commonly normal appearing white matter. This algorithm requires the segmentation of the specific tissue and is available on Git Hub. Shinohara et al 2014 ComBat is an MR harmonization tool originating from the field of genomics and is used in normalizing multi-site fMRI datasets and DTI. It achieves promising results. Fortin et al 2018 Z-Score Standardization versus Percentile NormalizationBoth techniques consider the variations within the image volumes and try to find commonalities such as statistical measures e.g. mean, std, and percentile. The z-score using statistical properties — mean and std — is less prone to larger outliers. However, variations in the distributions have an impact. For example when dealing with different pulse sequences such as e.g. T1, T2, and FLAIR. Their histograms vary greatly in their shape. In the example above, where the same scanner and pulse sequence are used, the histogram follows similar distribution but due to variations e.g. skull stripping, the outer tails might differ. Z-score standardization outperforms the shown percentile [2th-98th] normalization. Even though not perfect, both techniques perform better than the Min-Max normalization.Histograms of different normalization techniques for 3 different subjects. The dotted lines are for guiding the eye and correspond to the three peaks observed in Subject A (blue). Those are three randomly selected subjects however, the histograms of the subjects in the dataset vary as can be seen below.Those Histograms come from different subjects. The dotted lines belong to the most common tissue type present for Subject D. In the grid below, an MR slice around the ventricles is visualized. This time, the intensity range in matplotlib is set to a fixed range depending on the normalization strategy. It is an attempt to demonstrate how the computer receives the data. For Original the range is set to [0, 6000], for Min-Max, it is [0,1]; for Z-Score, we chose [-2.5,3]; and for Percentile (2–98), we chose [-0.15,1.2]. The chosen range influence brightness, by adjusting the range we can receive brighter or darker images. However, the consistency among different subjects is not impacted. We see that within Percentile and Z-Score, the intensity is more consistent compared to Min-MaxOther Normalization Techniques: Histogram matching, White stripe, ComBat Other Normalization Techniques (3–5), require additional software packages either available through libraries or GitHub. The normalization becomes a separate preprocessing step which sometimes might improve the results. Detailed analyzes and comparisons can be found in the reference section.Impact on Model’s Performance A general statement of the best normalization technique is hard to give as it depends on the particular problem. However, images acquired in the same center with the same protocol are already aligned well, differences might be small or introduced by other per-processing steps such as skull stripping. Nevertheless, it is a well-known problem that many models do not generalize well to data from other datasets. In Carré et al. a preprocessing pipeline for brain tumor radiomics is given, and they found out that Z-score combined with discretization outperformed White-Stripe and Histogram matching for T1w-gd and T2w-flair. In Chatterjee et al, they found that standardization improved the predictions when testing on data from a different site. Ghazvanchahi et al looked at the influence of different normalization schemes for WMH segmentation. They not only developed their own method but also showed that an ensemble combining models with different normalization techniques achieves significantly better results compared to the original model.Points to consider when choosing a normalization technique: Is there something special about my data, for example, the existence of contrast agent, mixing of different pulse sequences, a dataset that comes from multiple sites, and the performed preprocessing steps (e.g. skull stripping). All of that influences the data, the histograms, and hence the normalized images. When looking at the histogram, do the single metrics (aka mean, std, percentile) make sense to us,e or are more elaborate techniques needed? Normalization by Slice or by VolumeMRI data is usually volumetric in the sense that a subject contains a set of slices that can be combined to achieve a 3D volume or it is a true 3D MR acquisition (which often has isometric voxel dimensions) that can be split into single slices. When using 3D data for training, some might decide to train using slices instead of volumes as that significantly increases the size of the dataset. One might be tempted to normalize by slice but again, this introduces a bias. Not all tissues are represented within the same slice, the normalization by slices would lead to large variations in tissue intensity even for the same tissue type within the same subject. That can be avoided when normalizing by volume.Histograms of three slices within the brain. slice 110 has only little white matter contribution and therefore, any shown normalization scheme is unable to normalize it properly. Histogram Check ToolTo check the histograms and different normalization schemes for your own data, you can just upload 3–5 volumes in the tool below and they show you the histograms for the normalization techniques. The data is not stored or used in any way. It gets deleted once the tool is closed.Normalization Histogram — a Hugging Face Space by SusanneSchmidDiscover amazing ML apps made by the community huggingface.co ConclusionImage Normalization is one puzzle piece in your deep learning model. For a specific task and dataset, it might not impact your model performance significantly but for some of you, it might be the missing piece to improve the performance. In the provided tool, you can explore, normalization techniques on your own data and decided what technique is appropriate for your specific task. Acknowledgment:I would like to thank Wallace Loos for his valuable feedback and enriching discussions on different methodologies in the field of deep learning.Special thanks to ADNI for providing the data showcased here. References:[1] Petersen, Ronald Carl, et al. “Alzheimer’s disease neuroimaging initiative (ADNI): clinical characterization.” Neurology 74.3 (2010): 201–209. [2] Carré, Alexandre, et al. “Standardization of brain MR images across machines and protocols: bridging the gap for MRI-based radiomics.” Scientific reports 10.1 (2020): 12340. [3] Chatterjee, Avishek, et al. “Creating robust predictive radiomic models for data from independent institutions using normalization.” IEEE Transactions on Radiation and Plasma Medical Sciences 3.2 (2019): 210–215. [4] Fortin, Jean-Philippe, et al. “Harmonization of cortical thickness measurements across scanners and sites.” Neuroimage 167 (2018): 104–120. [5] Ghazvanchahi, Abdollah, et al. “Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI.” Medical Imaging with Deep Learning. 2023. [6] Nyúl, László G., and Jayaram K. Udupa. “On standardizing the MR image intensity scale.” Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine 42.6 (1999): 1072–1081. [7] Shinohara, Russell T., et al. “Statistical normalization techniques for magnetic resonance imaging.” NeuroImage: Clinical 6 (2014): 9–19.","link":"/2023/09/11/Intensity-Normalization-in-Medical-Imaging/"}],"tags":[],"categories":[],"pages":[{"title":"About","text":"That’s me.My name is Susanne and currently doing my PhD at the University of Calgary, Canada. I am interested in medical imaging techniques, in particular MRI, but also in deep learning and its interpretability. I hope to finish my Ph.D. soon to move forward to increase my portfolio and create a positive impact in my community.I hold a bachelor’s and master’s degree in physics from the Ludwigs-Maximilian University in Munich, Germany.I am an optimist overall. I like to contribute and be an active part of the group and come with plenty of ideas and enthusiasm. I am constantly driven to learn more and to go beyond my level of comfort. I am a critical thinker and can stand up for my thoughts if not convinced otherwise. CareerAfter finishing my master’s in Physics, I wanted to see more of the world and decided to start a PhD in Canada in a new field. My plans got disrupted when I had unpleasant experiences in my environment, I wanted to change and become my new me which can stay strong for myself, for my judgment, be open to seeing different perspectives, and create my own opinion.I also took on a new role: I am a mother of two kids. If you do not have kids, kids are a commitment greater than you can imagine 😊. Now, I am ready to move on: I want to share my skills and knowledge, to start a real career. About this side:For an educational challenge, I posted a blog post on a common website but discovered that it got hidden behind the paywall. I want to write for everyone, so, I took this opportunity to create my website. Contact Me.I am open to your feedback and happy to hear your thoughts on the posted articles. You also can also contact me via [email] (Susanne.Schmid@ucalgary.ca)","link":"/About/index.html"}]}